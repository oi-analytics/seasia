{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prompt-grounds",
   "metadata": {},
   "source": [
    "# Post-process ADM1 summary results\n",
    "\n",
    "For loading in oi-risk-vis interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclone_fnames = glob('../results/*/Cyclone/*.csv')\n",
    "fluvial_fnames = glob('../results/*/Fluvial/*/*.csv')\n",
    "coastal_fnames = glob('../results/*/Coastal/*/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_df(df, id_vars, new_id_vars):\n",
    "    # melt to get a long dataframe with \"variable\" and \"value\"\n",
    "    # columns, where \"variable\" is taken from each input column name\n",
    "    df_tidy = df \\\n",
    "        .reset_index() \\\n",
    "        .melt(id_vars=id_vars)\n",
    "    \n",
    "    # create a column from each new_id_var \n",
    "    # by splitting the \"variable\" names on \"_\"\n",
    "    for i, varname in enumerate(new_id_vars):\n",
    "        df_tidy[varname] = df_tidy.variable.apply(lambda d: d.split(\"_\")[i+1])\n",
    "        \n",
    "    # keep only the first element as the actual variable name \n",
    "    df_tidy['variable'] = df_tidy.variable.apply(lambda d: d.split(\"_\")[0])\n",
    "    \n",
    "    # parse an integer from return period, if present\n",
    "    if 'rp' in new_id_vars:\n",
    "        df_tidy.rp = df_tidy.rp.apply(lambda d: int(d.replace(\"RP\", \"\")))\n",
    "        \n",
    "    # pivot back out to columns for each variable\n",
    "    df_tidy = df_tidy.pivot_table(index=(id_vars + new_id_vars), columns='variable')\n",
    "    df_tidy.columns = [v for _, v in df_tidy.columns]\n",
    "    return df_tidy.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_rp_columns(df):\n",
    "    df_rp = df[[c for c in df.columns if 'RP' in c]]\n",
    "    df_exp = df[[c for c in df.columns if 'RP' not in c]]\n",
    "    return df_rp, df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_regroup(df):\n",
    "    rp, exp = split_df_by_rp_columns(df)\n",
    "    exp_tidy = tidy_df(\n",
    "        exp, \n",
    "        id_vars=['region_name', 'sector', 'rcp'],\n",
    "        new_id_vars=['gcm', 'epoch'])\n",
    "    rp_tidy = tidy_df(\n",
    "        rp, \n",
    "        id_vars=['region_name', 'sector', 'rcp'],\n",
    "        new_id_vars=['gcm', 'epoch', 'rp'])\n",
    "    return rp_tidy, exp_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-newfoundland",
   "metadata": {},
   "source": [
    "## Cyclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for fname in tqdm(cyclone_fnames):    \n",
    "    sector = os.path.basename(os.path.dirname(os.path.dirname(fname)))\n",
    "    df = pd.read_csv(fname)\n",
    "    df['sector'] = sector\n",
    "    dfs.append(df)\n",
    "cyclone = pd.concat(dfs).set_index(['sector','region_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclone_rp, cyclone_exp = split_df_by_rp_columns(cyclone)\n",
    "\n",
    "cyclone_rp = tidy_df(\n",
    "    cyclone_rp, \n",
    "    id_vars=['region_name', 'sector'], \n",
    "    new_id_vars=['hazard', 'rp'])\n",
    "cyclone_rp['rcp'] = 'historical'\n",
    "cyclone_rp['gcm'] = 'none'\n",
    "cyclone_rp['epoch'] = 'HIST'\n",
    "\n",
    "cyclone_exp = tidy_df(\n",
    "    cyclone_exp, \n",
    "    id_vars=['region_name', 'sector'], \n",
    "    new_id_vars=['hazard'])\n",
    "cyclone_exp['rcp'] = 'historical'\n",
    "cyclone_exp['gcm'] = 'none'\n",
    "cyclone_exp['epoch'] = 'HIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-thanks",
   "metadata": {},
   "source": [
    "## Flooding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = defaultdict(list)\n",
    "for fname in tqdm(fluvial_fnames):\n",
    "    climate = os.path.basename(os.path.dirname(fname))\n",
    "    sector = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(fname))))\n",
    "    df = pd.read_csv(fname)\n",
    "    df['sector'] = sector\n",
    "    df['rcp'] = climate\n",
    "    dfs[climate].append(df)\n",
    "fluvial_hist = pd.concat(dfs['historical']).set_index(['region_name','sector','rcp'])\n",
    "fluvial_fut  = pd.concat(dfs['rcp4p5'] + dfs['rcp8p5']).set_index(['region_name','sector','rcp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluvial_fut_rp, fluvial_fut_exp = split_regroup(fluvial_fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluvial_hist_rp, fluvial_hist_exp = split_regroup(fluvial_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluvial_rp = fluvial_hist_rp.append(fluvial_fut_rp)\n",
    "fluvial_rp.epoch.replace('1980', 'HIST', inplace=True) \n",
    "fluvial_rp['hazard'] = 'fluvial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluvial_exp = fluvial_hist_exp.append(fluvial_fut_exp)\n",
    "fluvial_exp.epoch.replace('1980', 'HIST', inplace=True) \n",
    "fluvial_exp['hazard'] = 'fluvial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-proof",
   "metadata": {},
   "source": [
    "## Coastal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = defaultdict(list)\n",
    "for fname in tqdm(coastal_fnames):\n",
    "    climate = os.path.basename(os.path.dirname(fname))\n",
    "    sector = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(fname))))\n",
    "    df = pd.read_csv(fname)\n",
    "    df['sector'] = sector\n",
    "    df['rcp'] = climate\n",
    "    dfs[climate].append(df)\n",
    "coastal_hist = pd.concat(dfs['historical']).set_index(['region_name','sector','rcp'])\n",
    "coastal_fut  = pd.concat(dfs['rcp4p5_perc_50'] + dfs['rcp8p5_perc_50']).set_index(['region_name','sector','rcp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_fut_rp, coastal_fut_exp = split_regroup(coastal_fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_hist_rp, coastal_hist_exp = split_regroup(coastal_hist)\n",
    "coastal_hist_rp = coastal_hist_rp[coastal_hist_rp.epoch == 'HIST']\n",
    "coastal_hist_exp = coastal_hist_exp[coastal_hist_exp.epoch == 'HIST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_rp = coastal_hist_rp.append(coastal_fut_rp)\n",
    "# above processing coded the subsidence scenario in the GCM column\n",
    "# pick only \"WS\" - with subsidence\n",
    "coastal_rp = coastal_rp[coastal_rp.gcm == 'WS'].copy()\n",
    "coastal_rp.gcm = 'none'\n",
    "coastal_rp.rcp = coastal_rp.rcp.str.replace('_perc_50', '')\n",
    "coastal_rp['hazard'] = 'coastal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_exp = coastal_hist_exp.append(coastal_fut_exp)\n",
    "# above processing coded the subsidence scenario in the GCM column\n",
    "# pick only \"WS\" - with subsidence\n",
    "coastal_exp = coastal_exp[coastal_exp.gcm == 'WS'].copy()\n",
    "coastal_exp.gcm = 'none'\n",
    "coastal_exp.rcp = coastal_exp.rcp.str.replace('_perc_50', '')\n",
    "coastal_exp['hazard'] = 'coastal'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-alcohol",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.concat([cyclone_exp, fluvial_exp, coastal_exp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = pd.concat([cyclone_rp, fluvial_rp, coastal_rp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.to_csv('../results/expected_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.to_csv('../results/rp_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-louisville",
   "metadata": {},
   "source": [
    "Some sense-checking of numbers of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rp.region_name.unique()) # total regions should be 312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.groupby(['hazard', 'rp']).count().reset_index()[['hazard', 'rp']].groupby('hazard').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.groupby(['hazard', 'sector', 'rcp', 'gcm', 'epoch']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "2940 / 10, 8736 / 28, 2808 / 9  # 294 is okay for coastal, excludes LAO regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.groupby(['hazard', 'sector', 'rcp', 'gcm', 'epoch']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-wilson",
   "metadata": {},
   "source": [
    "Historical - one row per region, sum over sectors and hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_exp = exp[exp.epoch=='HIST'] \\\n",
    "    .drop(\n",
    "        columns=[\n",
    "            'rcp', 'gcm', 'epoch', 'hazard', \n",
    "            'EAEL-primary-gdp', 'EAEL-secondary-gdp', 'EAEL-tertiary-gdp'\n",
    "        ]) \\\n",
    "    .groupby(['region_name']) \\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_exp = exp[exp.epoch=='2080'] \\\n",
    "    .groupby(['region_name', 'sector', 'hazard', 'rcp']) \\\n",
    "    .max() \\\n",
    "    .reset_index() \\\n",
    "    .drop(\n",
    "        columns=[\n",
    "            'gcm', 'epoch', 'hazard', \n",
    "            'EAEL-primary-gdp', 'EAEL-secondary-gdp', 'EAEL-tertiary-gdp'\n",
    "        ]) \\\n",
    "    .groupby(['region_name', 'rcp']) \\\n",
    "    .sum() \\\n",
    "    .reset_index() \\\n",
    "    .pivot(index='region_name', columns='rcp', values=['EAEL-gdp','expectedLengthDamaged','maxEAD','minEAD'])\n",
    "fut_exp.columns = [f\"{col}_{rcp}\" for col, rcp in fut_exp.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp = hist_exp.merge(fut_exp, on='region_name')\n",
    "all_exp.index.name = 'GID_1'\n",
    "all_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp.to_csv('../results/expected_summary.csv')\n",
    "len(all_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-adaptation",
   "metadata": {},
   "source": [
    "prob-loss direct/indirect - historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_summary(rp, region_name):\n",
    "    region_rp = rp[rp.region_name == region_name]\n",
    "    region_rp = region_rp[region_rp.epoch == 'HIST']\n",
    "    region_rp = region_rp[region_rp.rp.isin([10,50,100,500,1000])]\n",
    "    region_rp = region_rp[['rp', 'assetDamage', 'gdp']] \\\n",
    "        .groupby('rp') \\\n",
    "        .sum() \\\n",
    "        .reset_index()\n",
    "    # account for disruption\n",
    "    region_rp.gdp = region_rp.gdp * 30 / 365\n",
    "    \n",
    "    region_rp = region_rp \\\n",
    "        .rename(columns={\n",
    "            'gdp': '1. Economic impact',\n",
    "            'assetDamage': '2. Direct damages',\n",
    "        }) \\\n",
    "        .melt(id_vars='rp', var_name='field')\n",
    "    region_rp['probability'] = 1 / region_rp.rp\n",
    "    return region_rp.drop(columns='rp') \\\n",
    "        .sort_values(by=['field', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_name in tqdm(rp.region_name.unique()):\n",
    "    region_summary(rp, region_name).to_csv(f\"../results/historical_total/historical_total_{region_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = 'VNM.9_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_breakdown(rp, region_name):\n",
    "    region_rp = rp[rp.region_name == region_name].copy()\n",
    "    # account for disruption\n",
    "    region_rp.gdp = region_rp.gdp * 30 / 365\n",
    "    region_rp['probability'] = 1 / region_rp.rp\n",
    "\n",
    "    region_rp = region_rp.drop(columns=['region_name', 'primary-gdp', 'secondary-gdp', 'tertiary-gdp', 'rp', 'gcm'])\n",
    "    region_rp = region_rp.groupby(['sector', 'hazard', 'rcp', 'epoch', 'probability']) \\\n",
    "        .agg(['min', 'max'])\n",
    "    region_rp.columns = [f\"{var}_{agg}\" for var, agg in region_rp.columns]\n",
    "    return region_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_name in tqdm(rp.region_name.unique()):\n",
    "    region_breakdown(rp, region_name).to_csv(f\"../results/regional_breakdown/regional_breakdown_{region_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regional_total_by_climate(rp, region_name):\n",
    "    region_rp = rp[rp.region_name == region_name].copy()\n",
    "    # account for disruption\n",
    "    region_rp.gdp = region_rp.gdp * 30 / 365\n",
    "    region_rp['probability'] = 1 / region_rp.rp\n",
    "    region_rp = region_rp[region_rp.rp.isin([10,50,100,500,1000])]\n",
    "    region_rp = region_rp[region_rp.epoch.isin(['HIST', '2080'])]\n",
    "    region_rp = region_rp.drop(columns=['region_name', 'primary-gdp', 'secondary-gdp', 'tertiary-gdp', 'rp', 'gcm'])\n",
    "    region_rp = region_rp.groupby(['sector', 'hazard', 'rcp', 'epoch', 'probability']).max().reset_index()\n",
    "    region_rp = region_rp.groupby(['hazard', 'rcp', 'epoch', 'probability']).sum()\n",
    "    region_rp['totalDamage'] = region_rp.assetDamage + region_rp.gdp\n",
    "    return region_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_name in tqdm(rp.region_name.unique()):\n",
    "    regional_total_by_climate(rp, region_name).to_csv(f\"../results/climate_total/climate_total_{region_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_total_by_climate(rp, region_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
